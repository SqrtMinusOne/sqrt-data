#+TITLE: sqrt-data
#+PROPERTY: header-args:bash         :tangle-mode (identity #o755) :comments link :shebang "#!/usr/bin/env bash"
#+PROPERTY: header-args:python :comments link
#+PROPERTY: PRJ-DIR ..
#+HUGO_ALIASES: /sqrt-data

[[https://forthebadge.com/images/badges/works-on-my-machine.svg]]

This is an agglomeration of self-quantification scripts I've written over the years.

The basic ideas are as follows:
- I don't want to do a lot of (or any) manual work to collect the data. I'd rather have tools collect statistics in background so I could process them later.
- The tools have to be able to give away the data in machine-readable formats.
- Wherever possible, I want to own my data.

It's unlikely that you'll be to run the project as it is, as it's tuned pretty closely to my particular workflows and needs (hence the badge). Nevertheless, you may find something useful here.

The project is written with [[https://en.wikipedia.org/wiki/Literate_programming][literate programming]] paradigm with Emacs' [[https://orgmode.org/worg/org-contrib/babel/intro.html][Org Mode]] as a backend. This file contains basic information and common logic, necessary for the individual components.

* Architecture
TODO

* Python CLI
** Common API
This is my preferred way of writing Python modules. The =__init__.py= file imports all the required files with a star import, and the those files have the exported entities in the =__all__= variable.

That way I can import things like this: =from sqrt_data.api import settings=.

#+begin_src python :tangle (my/org-prj-dir "sqrt_data/api/__init__.py")
from .config import *
from .db import *
from .hash import *
from .hostname import *
#+end_src

*** Configuration
Let's start with configuration.

My configuration library of choice for Python is [[https://github.com/rochacbruno/dynaconf][dynaconf]]. I like it because it allows for some simple logic in the config files, e.g. dynamic variables.

#+begin_src python :tangle (my/org-prj-dir "sqrt_data/api/config.py")
import os

from dynaconf import Dynaconf

__all__ = ['settings']

settings = Dynaconf(
    settings_files=[
        'config.toml',
        os.path.expanduser('~/.config/sqrt-data/config.toml')
    ],
)
#+end_src

| Type | Note                                       |
|------+--------------------------------------------|
| TODO | Demarcate this block with links to modules |

#+begin_src conf-toml :tangle (my/org-prj-dir "config.toml")
[database]
user = 'postgres'
password = 'localdbpass'
database = 'data'
host = 'postgres'
port = 5432

[general]
root = '@format {env[HOME]}/logs-sync'
hash_db = '@format {this.general.root}/hash.db'
temp_data_folder = '/tmp/sqrt-data'

[archive]
days = 31
timeout = 5

[mpd]
library_csv = '@format {this.general.root}/mpd/mpd_library.csv'
log_folder = '@format {this.general.root}/mpd/logs'

[waka]
api_key = 'dummy'
api_url = 'https://wakatime.com/api/v1'
schema = 'wakatime'

[aw]
last_updated = '@format {this.general.root}/aw_last_updated.json'
logs_folder = '@format {this.general.root}/aw'
types = ['afkstatus', 'currentwindow', 'web.tab.current', 'app.editor.activity']
api = 'http://localhost:5600/api'
schema = 'aw'
skip_afk_interval = '900'
skip_afk_apps = '^(zoom|mpv)$'
skip_afk_titles = '^(YouTube)$'
webtab_apps = '^(Nightly)$'
skip_urls = '^(moz-extension|about:blank)'

[google]
android_file = '@format {this.general.root}/google/android-history.json'
android_schema = 'android'

[vk]
author = 'Pavel Korytov'
schema = 'vk'

[sleep]
file = '@format {this.general.root}/sleep/sleep-export.csv'
schema = 'sleep'

[sleep.geos]
e3336012 = 'test'

[location]
list_csv = '@format {this.general.root}/csv/locations.csv'
tz_csv = '@format {this.general.root}/csv/loc_timezones.csv'
hostnames_csv = '@format {this.general.root}/csv/loc_hostnames.csv'
#+end_src
*** Database
My favorite Python ORM framework is [[https://www.sqlalchemy.org/][SQLAlchemy]]. I've started this project before the 2.0 version was announced, so some of the usage may reflect the old patterns.

Also, I am using some actual ORM models here, but wherever I can, I trust pandas to manage tables for me.

I have a bunch of options of writing such files:
- if I use noweb, I can't use =M-x org-babel-detangle=
- if I write one huge block, I can't interweave the code with comments
- if I write small blocks, blocks with class methods would have to be indented and I can't use =C-c '=

So I opt for noweb. Fortunately, this seems to be the only class in the entire project, because I adopted it from another project which was designed with Object-Oriented Paradigm in mind.

This class stores the state in the class variables and all of its methods are static, so really it's nothing more than an aggregation of global variables. Now I'd make it singleton if I were to design this from scratch, but hey, it works.

#+begin_src python :noweb yes :tangle (my/org-prj-dir "sqrt_data/api/db.py")
import logging
from contextlib import contextmanager
from sqlalchemy import create_engine
from sqlalchemy.orm import scoped_session, sessionmaker

from .config import settings

__all__ = ['DBConn']


class DBConn:
    engine = None
    Session = None
    Base = None

    <<db-dbconn>>
#+end_src

A "constructor", which just sets up a bunch of class variables. This has to be called in before any call to the database.
#+begin_src python :noweb-ref db-dbconn :tangle no
def __init__(self, **kwargs):
    DBConn.engine = DBConn.get_engine(**kwargs)
    DBConn.Session = sessionmaker()
    DBConn.Session.configure(bind=self.engine)
    DBConn.scoped_session = scoped_session(DBConn.Session)
    logging.info('Initialized database connection')
#+end_src

A method to reset the class, just in case. The original project used this for unit tests, but I don't plan to implement unit tests here as of now.

#+begin_src python :noweb-ref db-dbconn :tangle no
@classmethod
def reset(cls):
    cls.engine = cls.Session = None
#+end_src

A method to get a database session object. In SQLAlchemy this seems to create transactions in the background, that is you can run =commit()=, =rollback()= and all that good stuff. By default, this commits automatically.
#+begin_src python :noweb-ref db-dbconn :tangle no
@staticmethod
@contextmanager
def get_session(**kwargs):
    session = DBConn.Session(**kwargs)
    yield session
    session.close()
#+end_src

The usage of the above methods is as follows:
#+begin_src python :tangle no
with DBConn.get_session() as db:
    db.<do-stuff>
#+end_src

A similar method, which can be used to ensure that a session exists.
#+begin_src python :noweb-ref db-dbconn :tangle no
@staticmethod
@contextmanager
def ensure_session(session, **kwargs):
    if session is None:
        session = DBConn.Session(**kwargs)
        yield session
        session.close()
    else:
        yield session
#+end_src

A method to get a fresh database engine. This object can be passed to pandas.
#+begin_src python :noweb-ref db-dbconn :tangle no
@staticmethod
def get_engine(user=None, password=None, **kwargs):
    url = "postgresql://{0}:{1}@{2}:{3}/{4}".format(
        user or settings.database.user, password or settings.database.password,
        settings.database.host, settings.database.port,
        settings.database.database
    )
    return create_engine(url, **kwargs)
#+end_src

Finally, a method to create tables in a schema.
#+begin_src python :noweb-ref db-dbconn :tangle no
@staticmethod
def create_schema(schema, Base=None):
    DBConn.engine.execute(f'CREATE SCHEMA IF NOT EXISTS {schema}')
    if Base is not None:
        tables = []
        for name, table in Base.metadata.tables.items():
            if table.schema == schema:
                tables.append(table)
        Base.metadata.create_all(DBConn.engine, tables)
#+end_src
*** Hashes
:PROPERTIES:
:header-args:python+: :tangle (my/org-prj-dir "sqrt_data/api/hash.py")
:END:

Another common thing I need is to track changes within files. The obvious way to do that is to compare hashes of files.
#+begin_src python
from sqlitedict import SqliteDict
import logging
import os
import subprocess
from .config import settings

__all__ = ['md5sum', 'HashDict']
#+end_src

So, first we need to calculate a hash.
#+begin_src python
def md5sum(filename):
    res = subprocess.run(
        ['md5sum', filename],
        capture_output=True,
        check=True,
        cwd=settings.general.root
    ).stdout
    res = res.decode('utf-8')
    return res.split(' ')[0]
#+end_src

Second, how do we actually store the hashes? I was using a huge JSON file for some time, but during the refactoring I've came to an idea that something like SQLite would make more sense. [[https://github.com/RaRe-Technologies/sqlitedict][SqliteDict]] seems to be a reasonable wrapper which does exactly what I want, so I'll use that as a base.

One note here is that the module crashes with message that =libgcc_s.so.1= cannot be found, which seems to be a problem with my Anaconda + Guix setup. [[https://stackoverflow.com/questions/64797838/libgcc-s-so-1-must-be-installed-for-pthread-cancel-to-work][This answer]] on StackOverflow has helped.
#+begin_src python
import ctypes
libgcc_s = ctypes.CDLL('libgcc_s.so.1')
#+end_src

Now, the class:
#+begin_src python
class HashDict(SqliteDict):
    def __init__(self, *args, **kwargs):
        super().__init__(settings.general.hash_db, *args, **kwargs)

    def is_updated(self, filename):
        saved = self.get(filename)
        return saved is None or saved != md5sum(filename)

    def save_hash(self, filename):
        self[filename] = md5sum(filename)

    def toggle_hash(self, filename):
        if self.is_updated(filename):
            self.save_hash(filename)
        else:
            self[filename] = '0'

    def report(self):
        for name, value in self.items():
            if os.path.exists(name):
                if self.is_updated(name):
                    print('[UPD]\t', end='')
                else:
                    print('[   ]\t', end='')
            else:
                print('[DEL]\t', end='')
            print(f"{value}\t{name}")
#+end_src
*** Hostname
We need a way to distinguish between machines on which the app is running. The easiest way to do that is via a hostname, but this doesn't work as expected on Android via termux, so there I have an environment variable set up.

#+begin_src python :tangle (my/org-prj-dir "sqrt_data/api/hostname.py")
import os
import socket

__all__ = ['get_hostname']


def get_hostname():
    return os.environ.get('ANDROID_PHONE', socket.gethostname())
#+end_src
** CLI entrypoint
:PROPERTIES:
:header-args:python+: :tangle (my/org-prj-dir "sqrt_data/manage.py")
:END:
We need an entrypoint for the CLI. My CLI library of choice is [[https://click.palletsprojects.com/en/8.0.x/][click]].

Also, [[https://github.com/magmax/python-inquirer][python-inquirer]] is nice library to query the user for something.
#+begin_src python
import logging

import click
import os
import inquirer

from sqrt_data.api import HashDict, settings, get_hostname
from sqrt_data import cli as cli_modules
#+end_src

A simple logging setup.
#+begin_src python
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S',
    handlers=[logging.FileHandler('./cli.log'),
              logging.StreamHandler()]
)
#+end_src

Initialize a click group.
#+begin_src python
@click.group()
def cli():
    print(f'CWD: {os.getcwd()}')
    print(f'hostname: {get_hostname()}')
#+end_src

Add all the components to the group.
#+begin_src python
cli.add_command(cli_modules.waka)
cli.add_command(cli_modules.android)
cli.add_command(cli_modules.vk)
cli.add_command(cli_modules.sleep)
cli.add_command(cli_modules.mpd)
cli.add_command(cli_modules.aw)
cli.add_command(cli_modules.locations)
cli.add_command(cli_modules.service)
#+end_src

The corresponding =__init__.py= in the CLI module:
#+begin_src python :tangle (my/org-prj-dir "sqrt_data/cli/__init__.py")
from .android import *
from .waka import *
from .vk import *
from .sleep import *
from .mpd import *
from .aw import *
from .locations import *
from .service import *
#+end_src

Two simple commands to work with file hashes.
#+begin_src python
@cli.command()
def hash_list():
    hashes = HashDict()
    hashes.report()


@cli.command()
@click.option('-n', '--name', required=False, type=str)
def hash_toggle(name):
    with HashDict() as h:
        if name is None:
            name = inquirer.prompt(
                [
                    inquirer.List(
                        'filename', 'Select filename', choices=list(h.keys())
                    )
                ]
            )['filename']  # type: ignore
        h.toggle_hash(os.path.join(settings.general.root, name))
        logging.info('Toggled hash for %s', name)
        h.commit()
#+end_src

Finally, to make this work, we have to invoke =cli()= in case =manage.py= is the main module. That is, when invoked with =python -m sqrt_data.manage=.
#+begin_src python
if __name__ == '__main__':
    cli()
#+end_src

To be able to invoke the app with =python -m sqrt_data=, the following =__main__.py= is necessary:
#+begin_src python :tangle (my/org-prj-dir "sqrt_data/__main__.py")
from .manage import cli

if __name__ == '__main__':
    cli()
#+end_src
** Misc
*** setup.py and requirements
#+begin_src python :tangle (my/org-prj-dir "setup.py")
from setuptools import find_packages, setup

setup(
    name='sqrt_data',
    version='2.0.1',
    description=
    'A collection of scripts to gather various data from my machines and store it on my VPS',
    author='SqrtMinusOne',
    author_email='thexcloud@gmail.com',
    packages=find_packages(),
    install_requires=[
        'pandas', 'numpy', 'click', 'inquirer', 'python-mpd2', 'sqlalchemy',
        'psycopg2-binary', 'requests', 'tqdm', 'beautifulsoup4', 'dynaconf',
        'sqlitedict'
    ],
    entry_points='''
    [console_scripts]
    sqrt_data=sqrt_data.manage:cli
    ''')
#+end_src

#+begin_src text :tangle (my/org-prj-dir "requirements.txt")
pandas
numpy
click
blessed==1.19.0
inquirer
python-mpd2
sqlalchemy
psycopg2-binary
requests
tqdm
beautifulsoup4
dynaconf
sqlitedict
schedule
#+end_src
* Client side
Individual machines have to save the data to the logs folder and propagate new files to the server.
** Installation details
As of now, until I made the repository public so I could write a Guix package or something, I just install the package in a conda environment.

Don't forget to copy config.toml to ~/.config/sqrt-data/config.toml

** Sync script
Here I use [[https://github.com/deajan/osync][osync]] to perform the syncronization. I even made a [[https://github.com/SqrtMinusOne/channel-q/blob/master/osync.scm][Guix package definition]], although didn't submit it yet.

Ideally I want this to be shipped with the application itself, so I'll definitely rewrite this in Python and make another CLI module at some point.

#+begin_src bash :tangle (my/org-prj-dir "scripts/sync-logs.sh")
PYTHON="/home/pavel/.conda/envs/data/bin/python"
CLI="-m sqrt_data"
DATA="$(hostname): $(date +"%Y-%m-%d")"
LOG_FILE="/home/pavel/logs-sync/sync.log"

TODAY_SYNC=$(grep -F "$DATA" $LOG_FILE)

if [ ! -z "$TODAY_SYNC" ] && [ "$1" != '-F' ]; then
    echo "Already synced today";
else
    $PYTHON $CLI mpd save-library
    $PYTHON $CLI aw save-buckets
    export RSYNC_EXCLUDE_PATTERN="sync.log"
    export CREATE_DIRS=yes
    export REMOTE_HOST_PING=false
    osync.sh --initiator=/home/pavel/logs-sync --target=ssh://pavel@sqrtminusone.xyz//home/pavel/logs-sync || exit 1
    echo "$(hostname): $(date +"%Y-%m-%d %H:%m")" >> $LOG_FILE
    export DISPLAY=:0
    notify-send "Syncronization" "Logs submitted to the server"
fi
#+end_src

The script is meant to be ran every hour. On GNU Guix I've created the following MCron job:
#+begin_src scheme
(job "0 * * * * " "~/Code/data/sqrt-data/scripts/sync-logs.sh")
#+end_src

* Server side
** Docker
The server part uses Docker, because I'm in love with Docker.

I need to run commands on schedule, and the easiest way to do that seems to be to use [[https://github.com/dbader/schedule][this Python package]]. I couldn't make cron work because of $PATH issues, but I didn't try too hard.

Also, I'm just fine with running the commands as suprocesses, because I already have a CLI.

#+begin_src python :tangle (my/org-prj-dir "tasks.py")
import time
import schedule
import subprocess


def waka_task():
    p = subprocess.run(['sqrt_data', 'waka', 'get-data'])
    if p.returncode != 0:
        return
    subprocess.run(['sqrt_data', 'waka', 'load'])


def mpd_task():
    p = subprocess.run(['sqrt_data', 'mpd', 'load-library'])
    if p.returncode != 0:
        return
    subprocess.run(['sqrt_data', 'mpd', 'load-logs'])


def sleep_task():
    subprocess.run(['sqrt_data', 'sleep', 'load'])


def aw_task():
    p = subprocess.run(['sqrt_data', 'aw', 'load'])
    if p.returncode != 0:
        return
    subprocess.run('sqrt_data', 'aw', 'preprocessing-dispatch')


schedule.every().day.at('00:00').do(waka_task)
schedule.every().day.at('01:00').do(mpd_task)
schedule.every().day.at('02:00').do(sleep_task)
schedule.every().day.at('03:00').do(aw_task)

while True:
    schedule.run_pending()
    time.sleep(1)
#+end_src

A Dockerfile for the program:
#+begin_src dockerfile :tangle (my/org-prj-dir "Dockerfile")
FROM python:3.10-buster
# Install sqrt-data
WORKDIR "sqrt_data/"
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY sqrt_data/ setup.py ./
RUN pip install .
ENV PYTHONPATH="$PYTHONPATH:/sqrt_data"

# Copy the configuration and apply the crontab
WORKDIR "/"
COPY tasks.py ./

# Run mcron
CMD python tasks.py
#+end_src

A docker-compose file. Consists of 4 services:
- PostgresSQL database
- Metabase
- This application
- A backup service
#+begin_src yaml :tangle (my/org-prj-dir "docker-compose.yml")
version: "3.5"

services:
    postgres:
        restart: unless-stopped
        image: postgres
        container_name: "sqrt-data-postgres"
        ports:
            - 5432:5432
        networks:
            - postgres
        environment:
            POSTGRES_USER: postgres
            POSTGRES_PASSWORD: localdbpass
            POSTGRES_DB: data
        volumes:
            - postgres_data:/data/postgres
    metabase:
        container_name: "sqrt-data-metabase"
        restart: unless-stopped
        image: metabase/metabase
        ports:
            - 8083:3000
        networks:
            - postgres
        depends_on:
            - postgres
        environment:
            MB_DB_TYPE: postgres
            MB_DB_DBNAME: metabase
            MB_DB_PORT: 5432
            MB_DB_USER: postgres
            MB_DB_PASS: localdbpass
            MB_DB_HOST: postgres
    sqrt_data:
        container_name: "sqrt-data"
        build: .
        restart: unless-stopped
        networks:
            - postgres
        depends_on:
            - postgres
        volumes:
            - type: bind
              source: ./config.toml
              target: /config.toml
            - type: bind
              source: ~/logs-sync-debug
              target: /root/logs-sync-debug
    backups:
        image: prodrigestivill/postgres-backup-local
        restart: always
        volumes:
            - ./backups:/backups
        networks:
            - postgres
        depends_on:
            - postgres
        environment:
            - POSTGRES_HOST=postgres
            - POSTGRES_DB=data,metabase
            - POSTGRES_USER=postgres
            - POSTGRES_PASSWORD=localdbpass
            - POSTGRES_EXTRA_OPTS=-Fc -Z9
            - SCHEDULE=@daily
            - BACKUP_KEEP_DAYS=7
            - BACKUP_KEEP_WEEKS=4
            - BACKUP_KEEP_MONTHS=2
            - BACKUP_SUFFIX=dump
            - HEALTHCHECK_PORT=8080

networks:
    postgres:
        driver: bridge

volumes:
    postgres_data:
#+end_src
** Basic deployement instructions
The server deployement is as follows.
1. Clone the repository
2. Change the settings in =config.toml= and =docker-compose.yml= according to the setup. What has to be changed:
   - The folder with logs has to be mounted to the container. The mount path has to be setup in =config.toml= in =general.root=.
   - =database.host= should be set to "postgres"
   - It makes sense to change password in =database.password= and in the compose file.
3. Create a database for metabase. To do that, run:
   #+begin_src bash :eval no
   docker-compose up postgres
   #+end_src

   Run psql in a separate shell:
   #+begin_src bash :eval no
   PGPASSWORD=localdbpass psql -h localhost -U postgres
   #+end_src

   And create a database:
   #+begin_src sql
   CREATE DATABASE metabase;
   #+end_src

   It may make sense to make a separate user for metabase here.
4. Check if everything works correctly.
   - Metabase instance should be available at http://localhost:8083/
   - Run =docker exec -it sqrt-data /bin/bash=:
     - Check if =config.toml= and the logs folder are mounted correctly
     - =sqrt_data hash-list= has to work
     - =sqrt_data mpd load-library= has to work
* Notes
** Android setup
Add to =.bashrc=:
#+begin_src bash
export ANDROID_PHONE="orchid"
#+end_src
