#+TITLE: sqrt-data
#+PROPERTY: header-args:bash         :tangle-mode (identity #o755) :comments link :shebang "#!/usr/bin/env bash"
#+PROPERTY: header-args:python :comments link
#+PROPERTY: PRJ-DIR ..
#+HUGO_ALIASES: /sqrt-data

[[https://forthebadge.com/images/badges/works-on-my-machine.svg]]

This is an agglomeration of self-quantification scripts I've written over the years.

The basic ideas are as follows:
- I don't want to do a lot of (or any) manual work to collect the data. I'd rather have tools collect statistics in background so I could process them later.
- The tools have to be able to give away the data in machine-readable formats.
- Wherever possible, I want to own my data.

It's unlikely that you'll be to run the project as it is, as it's tuned pretty closely to my particular workflows and needs (hence the badge). Nevertheless, you may find something useful here.

The project is written with [[https://en.wikipedia.org/wiki/Literate_programming][literate programming]] paradigm with Emacs' [[https://orgmode.org/worg/org-contrib/babel/intro.html][Org Mode]] as a backend. This file contains basic information and common logic, necessary for the individual components.

* Idea
The main idea is as follows:

- The data from various datasources is saved in machine-readable formats to a folder that gets rsynced to my VPS.
- A Docker container on that VPS runs cron jobs to process files in this folder and stores the data to a PostgreSQL database.
- [[https://www.metabase.com/][Metabase]] queries the data and presents me with nice charts and dashboards.

The entire thing is written in Python and more or less follows the path of least resistance. So solutions used here won't scale to the volumes of Big Data. Some may even need a rewrite in a few years if I somehow make it that far.

* Project structure
Here's a rough outline of the project structure and used data sources.

First, some files that aren't related to any particular data source:
- [[file:core.org][core.org]]. Parts of functionality used in the entire project: CLI, core API and deploy instructions.
- [[file:service.org][service.org]]. Some service things, for instance compressing old logs.

Second, files related to various datasources:
| File               | Data source                                            | Automation |
|--------------------+--------------------------------------------------------+------------|
| [[file:aw.org][aw.org]]             | [[https://activitywatch.net/][ActivityWatch]]                                          | Complete   |
| [[file:mpd.org][mpd.org]]            | [[https://www.musicpd.org/][Music Player Daemon]] (or my [[https://github.com/SqrtMinusOne/mpd-watcher][mpd-watcher]], to be precise) | Complete   |
| [[file:locations.org][locations.org]]      | My CSV with location history                           | Complete   |
| [[file:wakatime.org][wakatime.org]]       | [[https://wakatime.org][WakaTime]].                                              | Partial    |
| [[file:sleep.org][sleep.org]]          | [[https://sleep.urbandroid.org/][Sleep As Android]]                                       | Manual     |
| [[file:google-android.org][google-android.org]] | [[https://takeout.google.com/][Google Takeout]], Android activity                       | Manual     |
| [[file:vk.org][vk.org]]             | [[https://vk.com][VK]], GDPR dump                                          | Manual     |

"Automation" means:
- *Complete* - no manual actions required
- *Partial* - some manual action. Now that's only WakaTime, where I need to press "download dump" by myself
- *Manual* - manually retrieve the required data and feed it to the app
