#+TITLE: Sleep data
#+PROPERTY: header-args:python :comments link
#+PROPERTY: PRJ-DIR ..

Parsing the data from the [[https://sleep.urbandroid.org/][SleepAsAndroid]] app.

The data can be extracted from the app with the "Backup" option. The backup consists of three files, and the file with the actual data is called =sleep-export.csv=.

* Parse the data
:PROPERTIES:
:header-args:python: :tangle (my/org-prj-dir "sqrt_data/parse/sleep/load.py") :comments link
:END:

Import the required modules:
#+begin_src python
import copy
import logging
import re
from collections import deque
from datetime import datetime, timedelta

import pandas as pd

from sqrt_data.api import settings, DBConn, HashDict
#+end_src

The only exported function is =load=:
#+begin_src python
__all__ = ['load']
#+end_src

The structure the =sleep-export.csv= file is rather unusual. It is not a table, but an array of documents, stored in the following format:

| key_1   | key_2   | ... | key_n   |
| value_1 | value_2 | ... | value_n |

Where keys are not necessarily unique.

The first keys are common between all entires:
- =Id=
- =Tz= - timezone
- =From= - start timestamp in the =DD. MM. YYYY HH:mm= in format
- =To= - end timestamp in the same format
- =Sched= - not sure, but looks like a timestamp of falling asleep
- =Hours= - hours of sleep
- =Rating= - rating =[0, 0.25, ..., 5]=
- =Comment= - a string of the format =#tag1 #tag2 ... #tagN <text>=
- =Framerate=
- =Snore=
- =Noise=
- =Cycles= - number of sleep cycles
- =DeepSleep= - deep sleep percentage
- =LenAdjust= - ?
- =Geo= - internal geolocation tag

Then there are columns with key =HH:mm= and a float value, which looks like measured activity.

And finally there are columns with key =Event= and a value of a format =<event_name>-<timestamp>[-<value>]=.

Accordingly, I load the dump to three tables.

First, the key column and the numeric columns:
#+begin_src python
KEY_SEQ = 'Id'
NUM_COLS = set(
    [
        'Id', 'Hours', 'Rating', 'Framerate', 'Snore', 'Noise', 'Cycles',
        'DeepSleep', 'LenAdjust'
    ]
)
#+end_src

The date format:
#+begin_src python
DATE = '%d. %m. %Y %H:%M'
#+end_src

A function to extract tags and the actual comment from the =Comment= field:
#+begin_src python
def get_tags(comment):
    tags = re.findall('#\S+', comment)
    for i, tag in enumerate(tags):
        comment = comment.replace(tag, '')
        tags[i] = tag[1:]
    comment = comment.strip()
    return tags, comment
#+end_src

A function to parse one event:
#+begin_src python
def parse_event(event):
    values = event.split('-')
    return {
        'kind': values[0],
        'timestamp': int(values[1]),
        'time': datetime.utcfromtimestamp(int(values[1]) / 1000),
        'data': values[2:]
    }
#+end_src

And function to parse the entire structure:
#+begin_src python
def parse_csv_dict(lines):
    keys = None
    result = []
    for line in lines:
        if line.endswith('\n'):
            line = line[:-1]
        if keys is None:
            if line.startswith(KEY_SEQ):
                keys = line.split(',')
            else:
                continue
        else:
            row = {}
            events = []
            times = {}
            data = line.split(',')
            for key, datum in zip(keys, data):
                if datum.startswith('"'):
                    datum = datum[1:-1]
                if key.startswith('"'):
                    key = key[1:-1]
                if key == 'Event':
                    events.append(parse_event(datum))
                elif re.fullmatch(r'^\d+:\d+$', key):
                    times[key] = float(datum)
                elif key == 'From' or key == 'To' or key == 'Sched':
                    row[key] = datetime.strptime(datum, DATE)
                elif key == 'Id':
                    row['Id'] = int(datum)
                elif key in NUM_COLS:
                    row[key] = float(datum)
                elif key == 'Comment':
                    tags, comment = get_tags(datum)
                    row[key] = comment
                    row['tags'] = tags
                elif key == 'Geo':
                    row[key] = settings['sleep']['geos'].get(datum, datum)
                else:
                    row[key] = datum
            row['events'] = sorted(events, key=lambda evt: evt['timestamp'])
            row['times'] = times
            keys = None
            result.append(row)
    result = sorted(result, key=lambda datum: datum['From'])
    return result
#+end_src

I also want to merge entries the nearby entires:
#+begin_src python
def merge_data(data):
    data = copy.deepcopy(data)
    i, k = 0, 1
    result = []
    while i < len(data) - 1:
        a = data[i]
        b = data[i + k]
        if (b['From'] - a['To']) < timedelta(seconds=60 * 20):
            logging.info('Merged %s %s', b['From'], a['To'])
            data[i] = {
                'merged':
                    True,
                'Comment':
                    b['Comment'],
                'Cycles':
                    a['Cycles'] + b['Cycles'],
                'DeepSleep':
                    (a['DeepSleep'] * a['Hours'] + b['DeepSleep'] * b['Hours'])
                    / (a['Hours'] + b['Hours']),
                'Framerate':
                    b['Framerate'],
                'From':
                    a['From'],
                'Geo':
                    b['Geo'],
                'Hours':
                    a['Hours'] + b['Hours'],
                'Id':
                    a['Id'],
                'LenAdjust':
                    b['LenAdjust'],
                'Noise':
                    max(a['Noise'], b['Noise']),
                'Rating':
                    b['Rating'],
                'Sched':
                    a['Sched'],
                'Snore':
                    max(a['Snore'], b['Snore']),
                'To':
                    b['To'],
                'Tz':
                    b['Tz'],
                'events':
                    sorted(
                        [*a['events'], *b['events']],
                        key=lambda evt: evt['timestamp']
                    ),
                'tags':
                    list(set([*a['tags'], *b['tags']])),
                'times': {
                    ,**a['times'],
                    ,**b['times']
                }
            }
            k += 1
        else:
            result.append(data[i])
            i += k
            k = 1
    return result
#+end_src

And a function to convert the merged data to three DataFrames:
#+begin_src python
def get_dfs(data):
    data_main = deque()
    data_events = deque()
    data_times = deque()
    for datum in data:
        datum = {key.lower(): value for key, value in datum.items()}
        for event in datum['events']:
            data_events.append({**event, 'sleep_id': datum['id']})
        for time_, value in datum['times'].items():
            data_times.append(
                {
                    'time': datetime.strptime(time_, '%H:%M').time(),
                    'value': value,
                    'sleep_id': datum['id']
                }
            )
        del datum['events']
        del datum['times']
        data_main.append(datum)
    df_main, df_events, df_times = pd.DataFrame(data_main), pd.DataFrame(
        data_events
    ), pd.DataFrame(data_times)
    df_main['merged'] = df_main['merged'].apply(lambda d: d == True)
    df_main['cycles'] = df_main['cycles'].apply(lambda c: c if c > 0 else None)
    df_main['deepsleep'] = df_main['deepsleep'].apply(
        lambda d: d if d > 0 else None
    )
    return df_main, df_events, df_times
#+end_src

I want to put these DataFrames the database, but as there seems to be no way to create constraints with pandas, I do that manually:

#+begin_src sql :noweb-ref sleep-constraints
ALTER TABLE sleep.main DROP CONSTRAINT IF EXISTS main_pk;
ALTER TABLE sleep.main ADD CONSTRAINT main_pk PRIMARY KEY (id);
ALTER TABLE sleep.events DROP CONSTRAINT IF EXISTS events_sleep_fk;
ALTER TABLE sleep.times DROP CONSTRAINT IF EXISTS events_times_fk;
ALTER TABLE sleep.events ADD CONSTRAINT events_sleep_fk FOREIGN KEY (sleep_id) REFERENCES sleep.main(id);
ALTER TABLE sleep.times ADD CONSTRAINT times_sleep_fk FOREIGN KEY (sleep_id) REFERENCES sleep.main(id);
#+end_src

And a final function to perform the required operations:

#+begin_src python :noweb yes
CONSTRAINTS = """
<<sleep-constraints>>
"""

def load():
    schema = settings['sleep']['schema']
    DBConn()
    DBConn.engine.execute(f'DROP SCHEMA IF EXISTS {schema} CASCADE')
    DBConn.engine.execute(f'CREATE SCHEMA IF NOT EXISTS {schema}')

    with HashDict() as h:
        if not h.is_updated(settings['sleep']['file']):
            logging.info('Sleep alreay loaded')
            return
        with open(settings['sleep']['file'], 'r') as f:
            lines = f.readlines()

        data = parse_csv_dict(lines)
        logging.info('Parsed records: %d', len(data))
        data = merge_data(data)
        logging.info('Records after merge: %d', len(data))

        df_main, df_events, df_times = get_dfs(data)
        df_main = df_main.set_index('id')
        logging.info('Events: %d, Times: %d', len(df_events), len(df_times))

        df_main.to_sql(
            'main', schema=schema, con=DBConn.engine, if_exists='replace'
        )
        df_events.to_sql(
            'events', schema=schema, con=DBConn.engine, if_exists='replace'
        )
        df_times.to_sql(
            'times', schema=schema, con=DBConn.engine, if_exists='replace'
        )
        DBConn.engine.execute(CONSTRAINTS)
        h.save_hash(settings['sleep']['file'])
        h.commit()
#+end_src

* CLI
:PROPERTIES:
:header-args:python: :tangle (my/org-prj-dir "sqrt_data/cli/sleep.py") :comments link
:END:
The CLI interface via click.

#+begin_src python
import click
from sqrt_data.parse import sleep as sleep_
#+end_src

Export a click group named "sleep".
#+begin_src python
__all__ = ['sleep']

@click.group(help='Sleep stats')
def sleep():
    pass
#+end_src

The only command is to load the data to the database
#+begin_src python
@sleep.command(help='Load to DB')
def load():
    sleep_.load()
#+end_src
