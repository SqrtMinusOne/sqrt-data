#+TITLE: ActivityWatch stats
#+PROPERTY: header-args:python :comments link
#+PROPERTY: PRJ-DIR ..

[[https://activitywatch.net/][ActivityWatch]] is FOSS time tracker software.


* Models
The [[https://docs.activitywatch.net/en/latest/buckets-and-events.html][data model]] in the program is pretty reasonable. The top-level entry is called "bucket", which has the following attributes:
- =id=
- =created= - creation date
- =name=
- =type= - type of events in bucket
- =client= - ID of the client software
- =hostname=
and a list of events.

One event has the following attributes:
- =timestamp=
- =duration= - duration in seconds
- =data= - a dictionary with details about the event.

As of now, there are 4 default event types with the following structure of the =data= fields, which are quite self-descriptive:
- =afkstatus=
  - =status= - "afk" or "not-afk"
- =currentwindow=
  - =app=
  - =title=
- =app.editor.activity=
  - =file=
  - =project=
  - =language=
- =web.tab.current=
  - =url=
  - =title=
  - =audible=
  - =incognito=
  - =tabCount=

** Source models
So, I want to save this structure to my PostgreSQL database. I decided to break 3NF and use one table for each bucket type. So, here is a common model:

#+begin_src python :tangle (my/org-prj-dir "sqrt_data/models/aw/bucket.py")
import sqlalchemy as sa
from sqrt_data.models import Base

__all__ = ['Bucket']


class Bucket(Base):
    __table_args__ = {'schema': 'aw'}
    __abstract__ = True

    id = sa.Column(
        sa.String(256),
        primary_key=True,
    )
    bucket_id = sa.Column(sa.String(256), nullable=False)
    hostname = sa.Column(sa.String(256), nullable=False)
    timestamp = sa.Column(sa.DateTime(), nullable=False)
    duration = sa.Column(sa.Float(), nullable=False)
#+end_src

And here are the models for particular bucket types:
#+begin_src python :tangle (my/org-prj-dir "sqrt_data/models/aw/afkstatus.py")
import sqlalchemy as sa
from .bucket import Bucket

__all__ = ['AfkStatus']

class AfkStatus(Bucket):
    __tablename__ = 'afkstatus'
    __table_args__ = {'schema': 'aw'}

    status = sa.Column(sa.Boolean(), nullable=False)
#+end_src

#+begin_src python :tangle (my/org-prj-dir "sqrt_data/models/aw/currentwindow.py")
import sqlalchemy as sa
from .bucket import Bucket

__all__ = ['CurrentWindow']

class CurrentWindow(Bucket):
    __tablename__ = 'currentwindow'
    __table_args__ = {'schema': 'aw'}

    app = sa.Column(sa.Text(), nullable=False)
    title = sa.Column(sa.Text(), nullable=False)
#+end_src

#+begin_src python :tangle (my/org-prj-dir "sqrt_data/models/aw/appeditor.py")
import sqlalchemy as sa
from .bucket import Bucket

__all__ = ['AppEditor']

class AppEditor(Bucket):
    __tablename__ = 'appeditor'
    __table_args__ = {'schema': 'aw'}

    file = sa.Column(sa.Text(), nullable=False)
    project = sa.Column(sa.Text(), nullable=False)
    language = sa.Column(sa.Text(), nullable=False)
#+end_src

#+begin_src python :tangle (my/org-prj-dir "sqrt_data/models/aw/webtab.py")
import sqlalchemy as sa
from .bucket import Bucket

__all__ = ['WebTab']

class WebTab(Bucket):
    __tablename__ = 'webtab'
    __table_args__ = {'schema': 'aw'}

    url = sa.Column(sa.Text(), nullable=False)
    title = sa.Column(sa.Text(), nullable=False)
    audible = sa.Column(sa.Boolean(), nullable=False)
    incognito = sa.Column(sa.Boolean(), nullable=False)
    tab_count = sa.Column(sa.Integer(), nullable=True)
#+end_src

The corresponding =__init__.py=:
#+begin_src python :tangle (my/org-prj-dir "sqrt_data/models/aw/__init__.py")
from .bucket import *
from .afkstatus import *
from .currentwindow import *
from .appeditor import *
from .webtab import *
#+end_src
* Data
The corresponding =__init__.py=:

#+begin_src python :tangle (my/org-prj-dir "sqrt_data/parse/aw/__init__.py") :comments link
from .save import *
from .load import *
#+end_src
** Saving
:PROPERTIES:
:header-args:python: :tangle (my/org-prj-dir "sqrt_data/parse/aw/save.py") :comments link
:END:
As usual, first we need to export the data from ActivityWatch to csv format.

The required imports:
#+begin_src python
import socket
import json
import logging
import os
from collections import deque
from datetime import datetime

import pandas as pd
import requests

from sqrt_data.api import settings, get_hostname
#+end_src

The only exported function is =save_buckets=:
#+begin_src python
__all__ = ['save_buckets']
#+end_src

Buckets have a lot of data, so we need to somehow save the point of last download. That seems to be fine to persist in a JSON file.
#+begin_src python
def get_last_updated():
    data = {}
    if os.path.exists(os.path.expanduser(settings['aw']['last_updated'])):
        with open(os.path.expanduser(settings['aw']['last_updated']), 'r') as f:
            data = json.load(f)
    # return data.get(f'last_updated-{get_hostname()}', None)
    return data


def save_last_updated(data):
    os.makedirs(
        os.path.dirname(os.path.expanduser(settings['aw']['last_updated'])),
        exist_ok=True
    )
    data[f'last_updated-{get_hostname()}'] = datetime.now().isoformat()
    with open(os.path.expanduser(settings['aw']['last_updated']), 'w') as f:
        json.dump(data, f)
#+end_src

Next, get the data from the bucket and put it to a DataFrame:
#+begin_src python
def get_data(bucket_id, last_updated=None):
    params = {}
    api = settings['aw']['api']
    if last_updated:
        params['start'] = last_updated
    r = requests.get(f'{api}/0/buckets/{bucket_id}')
    bucket = r.json()
    r = requests.get(f'{api}/0/buckets/{bucket_id}/events', params=params)
    data = deque()
    for event in r.json():
        hostname = bucket['hostname']
        if hostname == 'unknown':
            hostname = get_hostname()
        data.append(
            {
                'id': f"{bucket_id}-{event['id']}",
                'bucket_id': bucket['id'],
                'hostname': bucket['hostname'],
                'duration': event['duration'],
                'timestamp': pd.Timestamp(event['timestamp']),
                **event['data']
            }
        )
    if len(data) > 0:
        df = pd.DataFrame(data)
        df = df.set_index('id')
        return df
    return None
#+end_src

Finally, a function to perform this operation on all the available buckets. I also want to function to run once per day to avoid creating too many files, so there is a simple limiter.
#+begin_src python
def save_buckets(force=False):
    last_updated = get_last_updated()
    last_updated_time = last_updated.get(f'last_updated-{get_hostname()}', None)
    if last_updated_time is not None:
        last_updated_date = datetime.fromisoformat(last_updated_time).date()
        if (datetime.now().date() == last_updated_date and not force):
            logging.info('Already loaded AW today')
            return
    r = requests.get(f'{settings["aw"]["api"]}/0/buckets')
    buckets = r.json()

    os.makedirs(
        os.path.expanduser(settings['aw']['logs_folder']), exist_ok=True
    )
    for bucket in buckets.values():
        if not bucket['type'] in settings['aw']['types']:
            continue
        if bucket['last_updated'] == last_updated.get(bucket['id'], None):
            logging.info('Bucket %s already saved', bucket['id'])
            continue
        df = get_data(bucket['id'], last_updated.get(bucket['id'], None))
        last_updated[bucket['id']] = bucket['last_updated']
        if df is None:
            logging.info('Bucket %s is empty', bucket['id'])
            continue
        bucket_type = bucket['type'].replace('.', '_')
        hostname = bucket['hostname']
        if hostname == 'unknown':
            hostname = get_hostname()
        filename = os.path.join(
            os.path.expanduser(settings['aw']['logs_folder']),
            f"{bucket_type}-{hostname}-{bucket['last_updated']}.csv"
        )
        df.to_csv(filename)
        logging.info('Saved %s with %s events', filename, len(df))
    save_last_updated(last_updated)
#+end_src
** Loading
:PROPERTIES:
:header-args:python: :tangle (my/org-prj-dir "sqrt_data/parse/aw/load.py") :comments link
:END:
The required imports:
#+begin_src python
import glob
import pandas as pd
import os
import re
import logging

from sqlalchemy.dialects.postgresql import insert
from tqdm import tqdm

from sqrt_data.api import settings, DBConn, HashDict
from sqrt_data.models import Base
from sqrt_data.models.aw import AfkStatus, CurrentWindow, AppEditor, WebTab
#+end_src

The only exported function is the one that performs the loading:
#+begin_src python
__all__ = ['load']
#+end_src

Get all the dataframes to load:
#+begin_src python
def get_dataframes(h):
    files = glob.glob(
        f'{os.path.expanduser(settings["aw"]["logs_folder"])}/*.csv'
    )
    dfs_by_type = {}
    for f in files:
        if not h.is_updated(f):
            continue
        try:
            df = pd.read_csv(f, lineterminator='\n', index_col=False)
        except pd.errors.ParserError:
            logging.error(f'Error parsing file: {f}')
            continue
        type_ = re.search(r'^\w+', os.path.basename(f)).group(0)
        try:
            dfs_by_type[type_].append(df)
        except KeyError:
            dfs_by_type[type_] = [df]
        h.save_hash(f)
    return dfs_by_type
#+end_src

Models by type:
#+begin_src python
MODELS = {
    'afkstatus': AfkStatus,
    'currentwindow': CurrentWindow,
    'app_editor_activity': AppEditor,
    'web_tab_current': WebTab
}
#+end_src

#+begin_src python
def get_records(type_, df):
    if type_ == 'afkstatus':
        df['status'] = df['status'] == 'not-afk'
    if type_ == 'web_tab_current':
        df = df.rename({ 'tabCount': 'tab_count' }, axis=1)
    return df.to_dict(orient='records')
#+end_src

And perform the loading:
#+begin_src python
def load():
    DBConn()
    DBConn.create_schema('aw', Base)
    with HashDict() as h:
        dfs_by_type = get_dataframes(h)

        with DBConn.get_session() as db:
            for type_, dfs in tqdm(dfs_by_type.items()):
                for df in dfs:
                    entries = get_records(type_, df)
                    db.execute(insert(MODELS[type_]).values(entries).on_conflict_do_nothing())
            db.commit()
        h.commit()
#+end_src
* CLI
:PROPERTIES:
:header-args:python: :tangle (my/org-prj-dir "sqrt_data/cli/aw.py") :comments link
:END:
The CLI interface via click.

#+begin_src python
import click
from sqrt_data.parse import aw as aw_
#+end_src

Export a click group named "mpd".
#+begin_src python
__all__ = ['aw']

@click.group(help='ActivityWatch stats')
def aw():
    pass
#+end_src

Save the buckets:
#+begin_src python
@aw.command(help='Save ActivityWatch buckets')
@click.option('--force', '-f', is_flag=True)
def save_buckets(force):
    aw_.save_buckets(force)

@aw.command(help='Load ActivityWatch buckets')
def load():
    aw_.load()
#+end_src
