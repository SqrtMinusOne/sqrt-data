#+TITLE: Service
#+PROPERTY: header-args:python :comments link
#+PROPERTY: PRJ-DIR ..

* Service module
The corresponding =__init__.py=:

#+begin_src python :tangle (my/org-prj-dir "sqrt_data/service/__init__.py")
from .compress import *
from .sync import *
#+end_src
** Compression
:PROPERTIES:
:header-args:python: :tangle (my/org-prj-dir "sqrt_data/service/compress.py") :comments link
:END:
As the data gets transferred between machines via CSV files, it is reasonable to compress the old ones.

The required imports:
#+begin_src python
import json
import logging
import os
import pathlib
import subprocess
import time

import pandas as pd

from sqrt_data.api import settings, HashDict
#+end_src

The only exported function is the one that performs the compression
#+begin_src python
__all__ = ['compress']
#+end_src


So, first we get a group to which a particular file belongs. When the group is full, it will be compressed.

#+begin_src python
def get_date_group(timestamp):
    return timestamp // (60 * 60 * 24 * settings['archive']['days'])
#+end_src

A group is full if there is no change of a file with this timestamp appearing in this group. In accordance with this, a function to get files to archive:

#+begin_src python
def get_files_to_compress():
    with HashDict() as h:
        files = [f for f in h.keys() if os.path.exists(f)]

    df = pd.DataFrame(
        {
            "name": files,
            "date_group":
                [
                    get_date_group(pathlib.Path(f).stat().st_mtime)
                    for f in files
                ],
            "dir": [os.path.dirname(f) for f in files]
        }
    )

    current_date_group = get_date_group(time.time())
    current_date_group_delta = time.time(
    ) // (60 * 60 * 24) - current_date_group * settings['archive']['days']
    df = df[df.date_group != current_date_group]
    if current_date_group_delta <= settings['archive']['timeout']:
        df = df[df.date_group != current_date_group - 1]

    return [
        (date_group, dir, g.name.tolist())  # type: ignore
        for (date_group,
             dir), g in df.groupby(['date_group', 'dir'])  # type: ignore
        if dir not in settings['archive']['exclude_dirs']
    ]
#+end_src

And a function dispatch the operation. It archives the old files and deletes the removed ones from this hash dictionary.
#+begin_src python
def compress():
    groups = get_files_to_compress()
    if len(groups) == 0:
        logging.info('Nothing to archive')
        return

    for date_group, dir, files in groups:
        archive_name = f'{os.path.relpath(os.path.dirname(files[0]), os.path.expanduser(settings["general"]["root"])).replace("/", "_")}_{int(date_group)}.tar.gz'
        logging.info(
            'Creating archive %s with %d files', archive_name, len(files)
        )
        subprocess.run(
            [
                'tar', '-czvf', archive_name, '--remove-files',
                ,*[os.path.relpath(f, dir) for f in files]
            ],
            check=True,
            cwd=dir
        )
    with HashDict() as h:
        for f in list(h.keys()):
            if not os.path.exists(f):
                del h[f]
                logging.info('Removed %s from HashDict', f)
        h.commit()
#+end_src

* CLI
A CLI via click.

#+begin_src python :tangle (my/org-prj-dir "sqrt_data/cli/service.py")
import click
from sqrt_data import service as service_

__all__ = ['service']


@click.group(help='Service')
def service():
    pass


@service.command(help='Compress old files')
def compress():
    service_.compress()

@service.command(help='Sync logs')
@click.option('--force', '-f', is_flag=True)
def sync_logs(force):
    service_.sync_logs(force)
#+end_src
