#+TITLE: Service
#+PROPERTY: header-args:python :comments link
#+PROPERTY: PRJ-DIR ..

* Service module
The corresponding =__init__.py=:

#+begin_src python :tangle (my/org-prj-dir "sqrt_data/service/__init__.py")
from .compress import *
from .sync import *
#+end_src
** Compression
:PROPERTIES:
:header-args:python: :tangle (my/org-prj-dir "sqrt_data/service/compress.py") :comments link
:END:
As the data gets transferred between machines via CSV files, it is reasonable to compress the old ones.

The required imports:
#+begin_src python
import json
import logging
import os
import pathlib
import subprocess
import time

import pandas as pd

from sqrt_data.api import settings, HashDict
#+end_src

The only exported function is the one that performs the compression
#+begin_src python
__all__ = ['compress']
#+end_src


So, first we get a group to which a particular file belongs. When the group is full, it will be compressed.

#+begin_src python
def get_date_group(timestamp):
    return timestamp // (60 * 60 * 24 * settings['archive']['days'])
#+end_src

A group is full if there is no change of a file with this timestamp appearing in this group. In accordance with this, a function to get files to archive:

#+begin_src python
def get_files_to_compress():
    with HashDict() as h:
        files = [f for f in h.keys() if os.path.exists(f)]

    df = pd.DataFrame(
        {
            "name": files,
            "date_group":
                [
                    get_date_group(pathlib.Path(f).stat().st_mtime)
                    for f in files
                ],
            "dir": [os.path.dirname(f) for f in files]
        }
    )

    current_date_group = get_date_group(time.time())
    current_date_group_delta = time.time(
    ) // (60 * 60 * 24) - current_date_group * settings['archive']['days']
    df = df[df.date_group != current_date_group]
    if current_date_group_delta <= settings['archive']['timeout']:
        df = df[df.date_group != current_date_group - 1]

    return [
        (date_group, dir, g.name.tolist())  # type: ignore
        for (date_group,
             dir), g in df.groupby(['date_group', 'dir'])  # type: ignore
        if dir not in settings['archive']['exclude_dirs']
    ]
#+end_src

And a function dispatch the operation. It archives the old files and deletes the removed ones from this hash dictionary.
#+begin_src python
def compress():
    groups = get_files_to_compress()
    if len(groups) == 0:
        logging.info('Nothing to archive')
        return

    for date_group, dir, files in groups:
        archive_name = f'{os.path.relpath(os.path.dirname(files[0]), os.path.expanduser(settings["general"]["root"])).replace("/", "_")}_{int(date_group)}.tar.gz'
        logging.info(
            'Creating archive %s with %d files', archive_name, len(files)
        )
        subprocess.run(
            [
                'tar', '-czvf', archive_name, '--remove-files',
                ,*[os.path.relpath(f, dir) for f in files]
            ],
            check=True,
            cwd=dir
        )
    with HashDict() as h:
        for f in list(h.keys()):
            if not os.path.exists(f):
                del h[f]
                logging.info('Removed %s from HashDict', f)
        h.commit()
#+end_src

** Sync
:PROPERTIES:
:header-args:python+: :tangle (my/org-prj-dir "sqrt_data/service/sync.py")
:END:
Syncronizing the =logs-sync= folder between client machines and the server. Previously, this was a bash script, but I decided to make it a proper Python module to ease making this a Guix package.

I use [[https://github.com/deajan/osync][osync]] to perform the syncronization. I even made a [[https://github.com/SqrtMinusOne/channel-q/blob/master/osync.scm][Guix package definition]], although didn't submit it yet.

#+begin_src python
import os
import subprocess
from datetime import datetime

from sqrt_data.api import settings, get_hostname, is_android
from sqrt_data.parse import aw, mpd
#+end_src

To make packaging easier, all CLI apps are written down like this:
#+begin_src python
EXEC_OSYNC = 'osync.sh'
EXEC_NOTIFY_SEND = 'notify-send'
#+end_src
The idea being that names will be replaced with full paths in the recipe.

The only exported function is one to perform the sync.
#+begin_src python
__all__ = ['sync_logs']
#+end_src

I want the sync be performed only once a day on a given hostname. Thus, the following string has to be written to the =sync.log_file= after a successful syncronization:
#+begin_src python
def log_string():
    date_string = datetime.strftime(datetime.now(), "%Y-%m-%d")
    return f'{get_hostname()}: {date_string}'
#+end_src

A function to check if that string exists in the file:
#+begin_src python
def check_today_sync():
    if not os.path.exists(settings.sync.log_file):
        return False
    string = log_string()
    with open(settings.sync.log_file, 'r') as f:
        for line in f:
            if line.strip() == string:
                return True
    return False
#+end_src

And a function to append that string to the file:
#+begin_src python
def set_today_sync():
    with open(settings.sync.log_file, 'a') as f:
        f.write(log_string() + '\n')
#+end_src

Now, performing the actual sync.
#+begin_src python
def sync_logs(force=False):
    if not force and check_today_sync():
        print('Already synced today!')
        return
    mpd.save_library()
    aw.save_buckets(force)
    subprocess.run(
        [
            EXEC_OSYNC, f'--initiator={settings.general.root}',
            f'--target={settings.sync.target}'
        ],
        env={
            **os.environ,
            'RSYNC_EXCLUDE_PATTERN': 'sync.log',
            'CREATE_DIRS': 'yes',
            'REMOTE_HOST_PING': 'false',
            'PATH': os.environ['PATH']
        },
        check=True
    )
    if not is_android():
        subprocess.run(
            [EXEC_NOTIFY_SEND, 'Sync', 'Logs submitted to the server'],
            env={'DISPLAY': ':0', **os.environ}
        )
    set_today_sync()
#+end_src

* CLI
A CLI via click.

#+begin_src python :tangle (my/org-prj-dir "sqrt_data/cli/service.py")
import click
from sqrt_data import service as service_

__all__ = ['service']


@click.group(help='Service')
def service():
    pass


@service.command(help='Compress old files')
def compress():
    service_.compress()

@service.command(help='Sync logs')
@click.option('--force', '-f', is_flag=True)
def sync_logs(force):
    service_.sync_logs(force)
#+end_src
