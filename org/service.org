#+TITLE: Service
#+PROPERTY: header-args:python :comments link
#+PROPERTY: PRJ-DIR ..

* Service module
The corresponding =__init__.py=:
#+begin_src python :tangle (my/org-prj-dir "sqrt_data/service/__init__.py")
from .compress import *
from .sync import *
#+end_src
** Compression
:PROPERTIES:
:header-args:python: :tangle (my/org-prj-dir "sqrt_data/service/compress.py") :comments link
:END:
As the data gets transferred between machines via CSV files, it is reasonable to compress the old ones.

The required imports:
#+begin_src python
import json
import logging
import os
import pathlib
import subprocess
import time

import pandas as pd

from sqrt_data.api import settings, HashDict
#+end_src

The only exported function is the one that performs the compression
#+begin_src python
__all__ = ['compress']
#+end_src


So, first we get a group to which a particular file belongs. When the group is full, it will be compressed.

#+begin_src python
def get_date_group(timestamp):
    return timestamp // (60 * 60 * 24 * settings['archive']['days'])
#+end_src

A group is full if there is no change of a file with this timestamp appearing in this group. In accordance with this, a function to get files to archive:

#+begin_src python
def get_files_to_compress():
    with HashDict() as h:
        files = [f for f in h.keys() if os.path.exists(f)]

    df = pd.DataFrame(
        {
            "name": files,
            "date_group":
                [
                    get_date_group(pathlib.Path(f).stat().st_mtime)
                    for f in files
                ],
            "dir": [os.path.dirname(f) for f in files]
        }
    )

    current_date_group = get_date_group(time.time())
    current_date_group_delta = time.time(
    ) // (60 * 60 * 24) - current_date_group * settings['archive']['days']
    df = df[df.date_group != current_date_group]
    if current_date_group_delta <= settings['archive']['timeout']:
        df = df[df.date_group != current_date_group - 1]

    return [
        (date_group, dir, g.name.tolist())  # type: ignore
        for (date_group,
             dir), g in df.groupby(['date_group', 'dir'])  # type: ignore
    ]
#+end_src

And a function dispatch the operation. It archives the old files and deletes the removed ones from this hash dictionary.
#+begin_src python
def compress():
    groups = get_files_to_compress()
    if len(groups) == 0:
        logging.info('Nothing to archive')
        return

    for date_group, dir, files in groups:
        archive_name = f'{os.path.relpath(os.path.dirname(files[0]), os.path.expanduser(settings["general"]["root"])).replace("/", "_")}_{int(date_group)}.tar.gz'
        logging.info(
            'Creating archive %s with %d files', archive_name, len(files)
        )
        subprocess.run(
            [
                'tar', '-czvf', archive_name, '--remove-files',
                ,*[os.path.relpath(f, dir) for f in files]
            ],
            check=True,
            cwd=dir
        )
    with HashDict() as h:
        for f in list(h.keys()):
            if not os.path.exists(f):
                del h[f]
                logging.info('Removed %s from HashDict', f)
        h.commit()
#+end_src

** Sync
:PROPERTIES:
:header-args:python+: :tangle (my/org-prj-dir "sqrt_data/service/sync.py")
:END:
Syncronizing the =logs-sync= folder between client machines and the server. Previously, this was a bash script, but I decided to make it a proper Python module to ease making this a Guix package.

I use [[https://github.com/deajan/osync][osync]] to perform the syncronization. I even made a [[https://github.com/SqrtMinusOne/channel-q/blob/master/osync.scm][Guix package definition]], although didn't submit it yet.

#+begin_src python
import os
import subprocess
from datetime import datetime

from sqrt_data.api import settings, get_hostname
#+end_src

To make packaging easier, all the CLI apps are written down like this:
#+begin_src python
EXEC_GREP = 'grep'
EXEC_OSYNC = 'osync.sh'
EXEC_NOTIFY_SEND = 'notify-send'
#+end_src
The idea being that names will be replaced with a full path in the recipe.

The only exported function is one to perform the sync.
#+begin_src python
__all__ = ['sync_logs']
#+end_src

I want the sync be performed only once a day on a given hostname. Thus, the following string has to be written to the =sync_log_file= after a successful syncronization:
#+begin_src python
def log_string():
    date_string = datetime.strftime(datetime.now(), "%Y-%m-%d")
    return f'{get_hostname()}: {date_string}'
#+end_src

A function to check if that string exists in the file:
#+begin_src python
def check_today_sync():
    result = subprocess.run(
        [EXEC_GREP, '-F',
         log_string(), settings.general.sync_log_file]
    )
    return result.returncode == 0
#+end_src

#+begin_src python
def sync_logs(force=False):
    if not force and check_today_sync():
        print('Already synced today!')
        return
#+end_src

* CLI
A CLI via click.

#+begin_src python :tangle (my/org-prj-dir "sqrt_data/cli/service.py")
import click
from sqrt_data import service as service_

__all__ = ['service']


@click.group(help='Service')
def service():
    pass


@service.command(help='Compress old files')
def compress():
    service_.compress()

@service.command()
@click.option('--force', '-f', is_flag=True, help='Sync logs')
def sync_logs(force):
    service_.sync_logs(force)
#+end_src
