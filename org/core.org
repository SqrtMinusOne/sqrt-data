#+TITLE: sqrt-data core
#+PROPERTY: header-args:bash         :tangle-mode (identity #o755) :comments link :shebang "#!/usr/bin/env bash"
#+PROPERTY: header-args:python :comments link :eval no
#+PROPERTY: header-args:scheme :comments link :eval no
#+PROPERTY: PRJ-DIR ..

This file contains the core API of the project and things related to the deployement.

* Python CLI
** Common API
This is my preferred way of writing Python modules. The =__init__.py= file imports all the required files with a star import, and those files have the exported entities in the =__all__= variable.

That way I can import things like this: =from sqrt_data.api import settings=.

#+begin_src python :tangle (my/org-prj-dir "sqrt_data/api/__init__.py")
from .config import *
from .db import *
from .hash import *
from .hostname import *
#+end_src

*** Configuration
Let's start with the configuration.

My configuration library of choice for Python is [[https://github.com/rochacbruno/dynaconf][dynaconf]]. I like it because it allows for some simple logic in the config files, e.g. dynamic variables.

#+begin_src python :tangle (my/org-prj-dir "sqrt_data/api/config.py")
import os

from dynaconf import Dynaconf

__all__ = ['settings']

settings_files = [
    # os.path.expanduser('~/.config/sqrt-data/config.toml'),
    'config.toml'
]

if all([not os.path.exists(f) for f in settings_files]):
    print('No config found!')

settings = Dynaconf(settings_files=settings_files)
#+end_src

| Type | Note                                       |
|------+--------------------------------------------|
| TODO | Demarcate this block with links to modules |

#+begin_src conf-toml :tangle (my/org-prj-dir "config.toml")
[database]
user = 'postgres'
password = 'localdbpass'
database = 'data'
host = 'postgres'
port = 5432

[general]
root = '@format {env[HOME]}/logs-sync'
hash_db = '@format {this.general.root}/hash.db'
temp_data_folder = '/tmp/sqrt-data'
cli_log = '@format {env[HOME]}/.local/share/sqrt-data/cli.log'

[sync]
log_file = '@format {this.general.root}/sync.log'
target = 'ssh://pavel@sqrtminusone.xyz//home/pavel/logs-sync'

[archive]
days = 31
timeout = 5

[mpd]
library_csv = '@format {this.general.root}/mpd/mpd_library.csv'
log_folder = '@format {this.general.root}/mpd/logs'

[waka]
api_key = 'dummy'
api_url = 'https://wakatime.com/api/v1'
schema = 'wakatime'

[aw]
last_updated = '@format {this.general.root}/aw_last_updated.json'
logs_folder = '@format {this.general.root}/aw'
android_file = '@format {this.general.root}/android-misc/aw-buckets-export.json'
types = ['afkstatus', 'currentwindow', 'web.tab.current', 'app.editor.activity']
api = 'http://localhost:5600/api'
schema = 'aw'
skip_afk_interval = '900'
skip_afk_apps = '^(zoom|mpv)$'
skip_afk_titles = '^(YouTube)$'
webtab_apps = '^(Nightly|firefox)$'
skip_urls = '^(moz-extension|about:blank)'

[aw.apps_convert]
Nightly = 'firefox'
Chromium-browser = 'Chromium'
unknown = 'Emacs' # EXWM

[google]
android_file = '@format {this.general.root}/google/android-history.json'
android_schema = 'android'

[youtube]
mpv_folder = '@format {this.general.root}/mpv'

[vk]
author = 'Pavel Korytov'
schema = 'vk'

[sleep]
file = '@format {this.general.root}/sleep/sleep-export.csv'
schema = 'sleep'

[sleep.geos]
e3336012 = 'test'

[location]
list_csv = '@format {this.general.root}/csv/locations.csv'
tz_csv = '@format {this.general.root}/csv/loc_timezones.csv'
hostnames_csv = '@format {this.general.root}/csv/loc_hostnames.csv'
#+end_src
*** Database
My favorite Python ORM framework is [[https://www.sqlalchemy.org/][SQLAlchemy]]. I've started this project before the 2.0 version was announced, so some of the usages may reflect the old patterns.

Also, I am using some actual ORM models here, but wherever I can, I trust pandas to manage tables for me.

I have a bunch of options for writing such files:
- if I use noweb, I can't use =M-x org-babel-detangle=
- if I write one huge block, I can't interweave the code with comments
- if I write small blocks, blocks with class methods would have to be indented and I can't use =C-c '=

So I opt for noweb. Fortunately, this seems to be the only class in the entire project, because I adopted it from another project which was designed with an Object-Oriented Paradigm in mind.

This class stores the state in the class variables and all of its methods are static, so really it's nothing more than an aggregation of global variables. Now I'd make it singleton if I were to design this from scratch, but hey, it works.

#+begin_src python :noweb yes :tangle (my/org-prj-dir "sqrt_data/api/db.py")
import logging
from contextlib import contextmanager
from sqlalchemy import create_engine
from sqlalchemy.orm import scoped_session, sessionmaker

from .config import settings

__all__ = ['DBConn']


class DBConn:
    engine = None
    Session = None
    Base = None

    <<db-dbconn>>
#+end_src

A "constructor", which just sets up a bunch of class variables. This has to be called in before any call to the database.
#+begin_src python :noweb-ref db-dbconn :tangle no
def __init__(self, **kwargs):
    DBConn.engine = DBConn.get_engine(**kwargs)
    DBConn.Session = sessionmaker()
    DBConn.Session.configure(bind=self.engine)
    DBConn.scoped_session = scoped_session(DBConn.Session)
    logging.info('Initialized database connection')
#+end_src

A method to reset the class, just in case. The original project used this for unit tests, but I don't plan to implement unit tests here as of now.

#+begin_src python :noweb-ref db-dbconn :tangle no
@classmethod
def reset(cls):
    cls.engine = cls.Session = None
#+end_src

A method to get a database session object. In SQLAlchemy this seems to create transactions in the background, that is you can run =commit()=, =rollback()= and all that good stuff. By default, this commits automatically.
#+begin_src python :noweb-ref db-dbconn :tangle no
@staticmethod
@contextmanager
def get_session(**kwargs):
    session = DBConn.Session(**kwargs)
    yield session
    session.close()
#+end_src

The usage of the above methods is as follows:
#+begin_src python :tangle no
with DBConn.get_session() as db:
    db.<do-stuff>
#+end_src

A similar method, which can be used to ensure that a session exists.
#+begin_src python :noweb-ref db-dbconn :tangle no
@staticmethod
@contextmanager
def ensure_session(session, **kwargs):
    if session is None:
        session = DBConn.Session(**kwargs)
        yield session
        session.close()
    else:
        yield session
#+end_src

A method to get a fresh database engine. This object can be passed to pandas.
#+begin_src python :noweb-ref db-dbconn :tangle no
@staticmethod
def get_engine(user=None, password=None, **kwargs):
    url = "postgresql://{0}:{1}@{2}:{3}/{4}".format(
        user or settings.database.user, password or settings.database.password,
        settings.database.host, settings.database.port,
        settings.database.database
    )
    return create_engine(url, **kwargs)
#+end_src

Finally, a method to create tables in a schema.
#+begin_src python :noweb-ref db-dbconn :tangle no
@staticmethod
def create_schema(schema, Base=None):
    DBConn.engine.execute(f'CREATE SCHEMA IF NOT EXISTS {schema}')
    if Base is not None:
        tables = []
        for name, table in Base.metadata.tables.items():
            if table.schema == schema:
                tables.append(table)
        Base.metadata.create_all(DBConn.engine, tables)
#+end_src
*** Hashes
:PROPERTIES:
:header-args:python+: :tangle (my/org-prj-dir "sqrt_data/api/hash.py")
:END:

Another common thing I need is to track changes within files. The obvious way to do that is to compare hashes of files.
#+begin_src python
from sqlitedict import SqliteDict
import logging
import os
import subprocess
from .config import settings

__all__ = ['md5sum', 'HashDict']
#+end_src

So, first we need to calculate a hash.
#+begin_src python
def md5sum(filename):
    res = subprocess.run(
        ['md5sum', filename],
        capture_output=True,
        check=True,
        cwd=settings.general.root
    ).stdout
    res = res.decode('utf-8')
    return res.split(' ')[0]
#+end_src

Second, how do we store the hashes? I was using a huge JSON file for some time, but during the refactoring, I've come to the idea that something like SQLite would make more sense. [[https://github.com/RaRe-Technologies/sqlitedict][SqliteDict]] seems to be a reasonable wrapper that does exactly what I want, so I'll use that as a base.

Edit <2022-01-08 Sat>: (TODO) I realized that there's little reason to store the hashes in a separate database. I should refactor that to store the hashes in the central database at some point.

One note here is that the module crashes with a message that =libgcc_s.so.1= cannot be found, which seems to be a problem with my Anaconda + Guix setup. [[https://stackoverflow.com/questions/64797838/libgcc-s-so-1-must-be-installed-for-pthread-cancel-to-work][This answer]] on StackOverflow has helped.
#+begin_src python
import ctypes
libgcc_s = ctypes.CDLL('libgcc_s.so.1')
#+end_src

Now, the class:
#+begin_src python
class HashDict(SqliteDict):
    def __init__(self, *args, **kwargs):
        super().__init__(settings.general.hash_db, *args, **kwargs)

    def is_updated(self, filename):
        saved = self.get(filename)
        return saved is None or saved != md5sum(filename)

    def save_hash(self, filename):
        self[filename] = md5sum(filename)

    def toggle_hash(self, filename):
        if self.is_updated(filename):
            self.save_hash(filename)
        else:
            self[filename] = '0'

    def report(self):
        for name, value in self.items():
            if os.path.exists(name):
                if self.is_updated(name):
                    print('[UPD]\t', end='')
                else:
                    print('[   ]\t', end='')
            else:
                print('[DEL]\t', end='')
            print(f"{value}\t{name}")
#+end_src
*** Hostname
We need a way to distinguish between machines on which the app is running. The easiest way to do that is via a hostname, but this doesn't work as expected on Android via termux, so there I have an environment variable set up.

#+begin_src python :tangle (my/org-prj-dir "sqrt_data/api/hostname.py")
import os
import socket

__all__ = ['get_hostname', 'is_android']

def is_android():
    return os.environ.get('ANDROID_PHONE') is not None

def get_hostname():
    return os.environ.get('ANDROID_PHONE', socket.gethostname())
#+end_src
** Cron tasks
Some tasks have to be run on schedule on both server and client. The easiest way to do that seems to be to use [[https://github.com/dbader/schedule][this Python package]].

#+begin_src python :tangle (my/org-prj-dir "sqrt_data/tasks/__init__.py")
from .server import *
from .client import *
#+end_src

*** Server
Tasks to be executed on a server on a regular basis. I'm using subprocesses because it's more stable here.

#+begin_src python :tangle (my/org-prj-dir "sqrt_data/tasks/server.py")
import time
import schedule
import subprocess

__all__ = ['run_server_cron']


def waka_task():
    p = subprocess.run(['sqrt_data', 'waka', 'get-data'])
    if p.returncode != 0:
        return
    subprocess.run(['sqrt_data', 'waka', 'load'])


def mpd_task():
    p = subprocess.run(['sqrt_data', 'mpd', 'load-library'])
    if p.returncode != 0:
        return
    subprocess.run(['sqrt_data', 'mpd', 'load-logs'])


def sleep_task():
    subprocess.run(['sqrt_data', 'sleep', 'load'])


def aw_task():
    p = subprocess.run(['sqrt_data', 'aw', 'load'])
    if p.returncode != 0:
        return
    subprocess.run(['sqrt_data', 'aw', 'postprocessing-dispatch'])


def archive_task():
    p = subprocess.run(['sqrt_data', 'service', 'compress'])
    if p.returncode != 0:
        return


def run_server_cron():
    schedule.every().day.at('00:00').do(waka_task)
    schedule.every().day.at('01:00').do(mpd_task)
    schedule.every().day.at('02:00').do(sleep_task)
    schedule.every().day.at('03:00').do(aw_task)
    schedule.every().day.at('04:00').do(archive_task)

    while True:
        schedule.run_pending()
        time.sleep(1)
#+end_src
*** Client
Client has just one task to sync logs to be executed every hour.

The actual sync happens only once a day, but this approach is an easy way to ensure that the sync will happen. E.g., if at one hour sync failed because there was no internet, it will retry the next hour.

#+begin_src python :tangle (my/org-prj-dir "sqrt_data/tasks/client.py")
import schedule
import time
import logging
from sqrt_data.service import sync_logs

__all__ = ['run_client_cron']


def client_task():
    try:
        sync_logs()
    except Exception:
        logging.exception('Sync error!')


def run_client_cron():
    schedule.every().hour.at(":00").do(client_task)
    while True:
        schedule.run_pending()
        time.sleep(1)
#+end_src
** CLI entrypoint
:PROPERTIES:
:header-args:python+: :tangle (my/org-prj-dir "sqrt_data/manage.py")
:END:
We need an entrypoint for the CLI. My CLI library of choice is [[https://click.palletsprojects.com/en/8.0.x/][click]].

Also, [[https://github.com/magmax/python-inquirer][python-inquirer]] is a nice library to query the user for something.
#+begin_src python
import logging

import click
import os
import inquirer

from sqrt_data.api import HashDict, settings, get_hostname
from sqrt_data import cli as cli_modules
from sqrt_data import tasks
#+end_src

A simple logging setup.
#+begin_src python
if 'general' in settings:
    if not os.path.exists(os.path.dirname(settings.general.cli_log)):
        os.mkdir(os.path.dirname(settings.general.cli_log))

    logging.basicConfig(
        level=logging.DEBUG,
        format='%(asctime)s %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S',
        handlers=[logging.FileHandler(settings.general.cli_log),
                  logging.StreamHandler()]
    )
else:
    print("Cannot initialize logging without config")
#+end_src

Initialize a click group.
#+begin_src python
@click.group()
def cli():
    print(f'CWD: {os.getcwd()}')
    print(f'hostname: {get_hostname()}')
#+end_src

Add all the components to the group.
#+begin_src python
cli.add_command(cli_modules.waka)
cli.add_command(cli_modules.android)
cli.add_command(cli_modules.vk)
cli.add_command(cli_modules.sleep)
cli.add_command(cli_modules.mpd)
cli.add_command(cli_modules.aw)
cli.add_command(cli_modules.locations)
cli.add_command(cli_modules.service)
cli.add_command(cli_modules.youtube)
#+end_src

The corresponding =__init__.py= in the CLI module:
#+begin_src python :tangle (my/org-prj-dir "sqrt_data/cli/__init__.py")
from .android import *
from .waka import *
from .vk import *
from .sleep import *
from .mpd import *
from .aw import *
from .locations import *
from .service import *
from .youtube import *
#+end_src

Two simple commands to work with file hashes.
#+begin_src python
@click.group(help='Working with hashes')
def hash():
    pass

@hash.command()
def hash_list():
    hashes = HashDict()
    hashes.report()


@hash.command()
@click.option('-n', '--name', required=False, type=str)
def hash_toggle(name):
    with HashDict() as h:
        if name is None:
            name = inquirer.prompt(
                [
                    inquirer.List(
                        'filename', 'Select filename', choices=list(h.keys())
                    )
                ]
            )['filename']  # type: ignore
        h.toggle_hash(os.path.join(settings.general.root, name))
        logging.info('Toggled hash for %s', name)
        h.commit()

cli.add_command(hash)
#+end_src

Also commands to initalize "cron jobs":
#+begin_src python
@click.group(help='Initialize recurring tasks. Meant to be run in a service or such')
def cron():
    pass


@cron.command()
def run_server_cron():
    tasks.run_server_cron()


@cron.command()
def run_client_cron():
    tasks.run_client_cron()


cli.add_command(cron)
#+end_src

Finally, to make this work, we have to invoke =cli()= in case =manage.py= is the main module. That is, when invoked with =python -m sqrt_data.manage=.
#+begin_src python
if __name__ == '__main__':
    cli()
#+end_src

To be able to invoke the app with =python -m sqrt_data=, the following =__main__.py= is necessary:
#+begin_src python :tangle (my/org-prj-dir "sqrt_data/__main__.py")
from .manage import cli

if __name__ == '__main__':
    cli()
#+end_src
* Install
Individual machines have to save the data to the logs folder and propagate new files to the server. Thus, the project has to be installed both on client machines and on the server.

For server installation, check [[*Server deployement][this section]]. For client machines, the installation options are as follows:
- install as a [[*Python][Python package]]
- install as a [[*Guix development setup][Guix package]] from the shipped definition
- install as a Guix package from [[https://github.com/SqrtMinusOne/channel-q][my Guix channel]]

I use the Python package for Android and Guix package for my desktop machines. For Android, also set the =ANDROID_PHONE= environment variable:
#+begin_src bash
export ANDROID_PHONE="orchid"
#+end_src

After installing the package:
- copy =config.toml= to =~/.config/sqrt-data/config.toml=
- run =sqrt_data cron run-client-cron=

** Python setup
To install the package, run:
#+begin_src bash :eval no
pip install .
#+end_src

To install the dependencies, run:
#+begin_src bash :eval no
pip install -r requirements.txt
#+end_src

*** setup.py
#+begin_src python :tangle (my/org-prj-dir "setup.py")
from setuptools import find_packages, setup

setup(
    name='sqrt_data',
    version='2.0.1',
    description=
    'A collection of scripts to gather various data from my machines and store it on my VPS',
    author='SqrtMinusOne',
    author_email='thexcloud@gmail.com',
    packages=find_packages(),
    install_requires=[
        'pandas', 'numpy', 'click', 'inquirer', 'python-mpd2', 'sqlalchemy',
        'psycopg2-binary', 'requests', 'tqdm', 'beautifulsoup4', 'dynaconf',
        'sqlitedict', 'furl', 'schedule', 'tldextract'
    ],
    entry_points='''
    [console_scripts]
    sqrt_data=sqrt_data.manage:cli
    ''')
#+end_src

*** requireiments.txt
#+begin_src text :tangle (my/org-prj-dir "requirements.txt")
pandas
numpy
click
blessed==1.19.0
inquirer
python-mpd2
sqlalchemy
psycopg2-binary
requests
tqdm
beautifulsoup4
dynaconf
sqlitedict
schedule
furl
tldextract
#+end_src
** Guix development setup
This part is largely inspired by the Nyxt build setup.

*** Usage
To make a development environment, run:
#+begin_src bash :eval no
guix shell --container -D -f sqrt-data.scm --share=$HOME/logs-sync
#+end_src
This will create the environment with all the dependencies, but not the =sqrt_data= package itself. =python -m sqrt_data= inside the environment should work.

To create an environment with the package, remove the =-D= flag:
#+begin_src bash :eval no
guix shell --container -f sqrt-data.scm --share=$HOME/logs-sync
#+end_src
Now, just =sqrt_data= should work inside the environment.

One issue with a container is that the app may not have access to stuff outside the container, like the MPD socket. If such access is necessary, remove the =--container= flag.
#+begin_src bash :eval no
guix shell -f sqrt-data.scm
#+end_src

*** Guix module
Defining the module.
#+begin_src scheme :tangle (my/org-prj-dir "sqrt-data.scm")
(define-module (sqrt-data)
  #:use-module (srfi srfi-1)
  #:use-module (srfi srfi-26)
  #:use-module (ice-9 match)
  #:use-module (ice-9 rdelim)
  #:use-module (ice-9 popen)
  #:use-module (guix download)
  #:use-module (guix git-download)
  #:use-module (guix gexp)
  #:use-module (guix packages)
  #:use-module (guix build utils)
  #:use-module (guix build-system python)
  #:use-module (guix build-system gnu)
  #:use-module ((guix licenses) #:prefix license:)
  #:use-module (gnu packages admin)
  #:use-module (gnu packages base)
  #:use-module (gnu packages compression)
  #:use-module (gnu packages databases)
  #:use-module (gnu packages gawk)
  #:use-module (gnu packages gnome)
  #:use-module (gnu packages mpd)
  #:use-module (gnu packages networking)
  #:use-module (gnu packages rsync)
  #:use-module (gnu packages python-web)
  #:use-module (gnu packages python-xyz)
  #:use-module (gnu packages python-science)
  #:use-module (gnu packages ssh)
  #:use-module (gnu packages version-control))
#+end_src

We want to build the package from the local source, so here is a way to figure out the source directory.
#+begin_src scheme :tangle (my/org-prj-dir "sqrt-data.scm")
(define %source-dir (dirname (current-filename)))
;; (define %source-dir "/home/pavel/Code/self-quantification/sqrt-data/")
#+end_src

Filter the list of files by =git ls-files=.
#+begin_src scheme :tangle (my/org-prj-dir "sqrt-data.scm")
(define git-file?
  (let* ((pipe (with-directory-excursion %source-dir
                 (open-pipe* OPEN_READ "git" "ls-files")))
         (files (let loop ((lines '()))
                  (match (read-line pipe)
                    ((? eof-object?)
                     (reverse lines))
                    (line
                     (loop (cons line lines))))))
         (status (close-pipe pipe)))
    (lambda (file stat)
      (match (stat:type stat)
        ('directory
         #t)
        ((or 'regular 'symlink)
         (any (cut string-suffix? <> file) files))
        (_
         #f)))))
#+end_src

Get the version of the package with =git describe --always --tags=.
#+begin_src scheme :tangle (my/org-prj-dir "sqrt-data.scm")
(define (git-version)
  (let* ((pipe (with-directory-excursion %source-dir
                 (open-pipe* OPEN_READ "git" "describe" "--always" "--tags")))
         (version (read-line pipe)))
    (close-pipe pipe)
    version))
#+end_src

Guix doesn't seem to have all the required dependencies, so here are ones that are missing from the official repository.
#+begin_src scheme :tangle (my/org-prj-dir "sqrt-data.scm")
(define-public python-readchar
  (package
    (name "python-readchar")
    (version "3.0.5")
    (source
     (origin
       (method url-fetch)
       (uri (pypi-uri "readchar" version))
       (sha256
        (base32 "1h42qjj9079yv19rd1zdl3wg915bg2wrvfjzd4xpyyy3k0gbgxfi"))))
    (arguments
     `(#:tests? #f))
    (build-system python-build-system)
    (propagated-inputs (list python-flake8))
    (home-page "https://github.com/magmax/python-readchar")
    (synopsis "Utilities to read single characters and key-strokes")
    (description "Utilities to read single characters and key-strokes")
    (license license:expat)))

(define-public python-blessed-1.19
  (package
    (inherit python-blessed)
    (version "1.19.0")
    (source
     (origin
       (method url-fetch)
       (uri (pypi-uri "blessed" version))
       (sha256
        (base32 "0qbsmnjwj016a0zg0jx9nnbfkzr700jlms18nlqa7bk1ax7gkc2d"))
       (modules '((guix build utils)))
       (snippet
        '(begin
           ;; Don't get hung up on Windows test failures.
           (delete-file "blessed/win_terminal.py") #t))))))

(define-public python-inquirer
  (package
    (name "python-inquirer")
    (version "2.9.1")
    (source
     (origin
       (method url-fetch)
       (uri (pypi-uri "inquirer" version))
       (sha256
        (base32 "0pdzkm52dz9fy67qpgivq99hfw5j5f3ry73pcpndgaxwm3maiw35"))))
    (build-system python-build-system)
    (propagated-inputs (list python-blessed-1.19 python-editor python-readchar))
    (arguments
     `(#:tests? #f))
    (home-page "https://github.com/magmax/python-inquirer")
    (synopsis
     "Collection of common interactive command line user interfaces, based on Inquirer.js")
    (description
     "Collection of common interactive command line user interfaces, based on
Inquirer.js")
    (license license:expat)))

(define-public python-sqlitedict
  (package
    (name "python-sqlitedict")
    (version "1.7.0")
    (source
     (origin
       (method url-fetch)
       (uri (pypi-uri "sqlitedict" version))
       (sha256
        (base32 "0mmdph6yrlynyyc88hdg0m12k4p3ppn029k925sxmm5c38qcrzra"))))
    (build-system python-build-system)
    (home-page "https://github.com/piskvorky/sqlitedict")
    (synopsis
     "Persistent dict in Python, backed up by sqlite3 and pickle, multithread-safe.")
    (description
     "Persistent dict in Python, backed up by sqlite3 and pickle, multithread-safe.")
    (license license:asl2.0)))
#+end_src

Also declaring osync here because I'm not sure how to include my channel.
#+begin_src scheme :tangle (my/org-prj-dir "sqrt-data.scm")
(define-public osync
  (package
    (name "osync")
    (version "1.3-beta3")
    (source
     (origin
       (method git-fetch)
       (uri (git-reference
             (url "https://github.com/deajan/osync/")
             (commit (string-append "v" version))))
       (file-name (git-file-name name version))
       (sha256
        (base32 "1zpxypgfj6sr87wq6s237fr2pxkncjb0w9hq14zfjppkvws66n0w"))))
    (build-system gnu-build-system)
    (arguments
     `(#:tests? #f
       #:validate-runpath? #f
       #:phases
       (modify-phases %standard-phases
         (add-after 'unpack 'patch-file-names
           (lambda _
             ;; Silence beta warining. Otherwise the exitcode is not zero
             (substitute* "osync.sh" (("IS_STABLE=false") "IS_STABLE=true"))))
         (delete 'bootstrap)
         (delete 'configure)
         (delete 'build)
         (replace 'install
           (lambda* (#:key outputs #:allow-other-keys)
             (let ((out (string-append (assoc-ref outputs "out"))))
               ;; Use system* because installer returns exitcode 2 because it doesn't find systemd or initrc
               (system* "./install.sh" (string-append "--prefix=" out) "--no-stats")
               (mkdir (string-append out "/bin"))
               (symlink (string-append out "/usr/local/bin/osync.sh")
                        (string-append out "/bin/osync.sh"))
               (symlink (string-append out "/usr/local/bin/osync-batch.sh")
                        (string-append out "/bin/osync-batch.sh"))
               (symlink (string-append out "/usr/local/bin/ssh-filter.sh")
                        (string-append out "/bin/ssh-filter.sh"))
               #t))))))
    ;; TODO replace the executables with full paths
    ;; XXX Can't put "iputils" in propagated-inputs because on Guix
    ;; "ping" is in setuid-programs. Set "REMOTE_HOST_PING" to false if ping
    ;; is not available.
    (propagated-inputs
     `(("rsync" ,rsync)
       ("gawk" ,gawk)
       ("coreutils" ,coreutils)
       ("openssh" ,openssh)
       ("gzip" ,gzip)
       ("hostname" ,inetutils)))
    (synopsis "A robust two way (bidirectional) file sync script based on rsync with fault tolerance, POSIX ACL support, time control and near realtime sync")
    (home-page "http://www.netpower.fr/osync")
    (license license:bsd-3)
    (description "A two way filesync script running on bash Linux, BSD, Android, MacOSX, Cygwin, MSYS2, Win10 bash and virtually any system supporting bash). File synchronization is bidirectional, and can be run manually, as scheduled task, or triggered on file changes in daemon mode. It is a command line tool rsync wrapper with a lot of additional features baked in.")))
#+end_src

Finally, the definition of the package.
#+begin_src scheme :tangle (my/org-prj-dir "sqrt-data.scm")
(define-public sqrt-data
  (package
    (name "sqrt-data")
    (version (git-version))
    (source
     (local-file %source-dir #:recursive? #t #:select? git-file?))
    (build-system python-build-system)
    (arguments
     `(#:tests? #f
       #:phases
       (modify-phases %standard-phases
         (add-before 'build 'fix-dependencies
           (lambda _
             (substitute* "setup.py" (("psycopg2-binary") "psycopg2"))
             (substitute* "sqrt_data/service/sync.py"
               (("EXEC_NOTIFY_SEND = (.*)")
                (format #f "EXEC_NOTIFY_SEND = ~s\n" (which "notify-send"))))
             (substitute* "sqrt_data/service/sync.py"
               (("EXEC_OSYNC = (.*)")
                (format #f "EXEC_OSYNC = ~s\n" (which "osync.sh")))))))))
    (native-inputs
     `(("git" ,git-minimal)))
    (inputs
     `(("libnotify" ,libnotify)
       ("osync" ,osync)))
    (propagated-inputs
     `(("python-pandas" ,python-pandas)
       ("python-numpy" ,python-numpy)
       ("python-click" ,python-click)
       ("python-inquirer", python-inquirer)
       ("python-mpd2" ,python-mpd2)
       ("python-sqlalchemy" ,python-sqlalchemy)
       ("python-psycopg2" ,python-psycopg2)
       ("python-requests" ,python-requests)
       ("python-tqdm" ,python-tqdm)
       ("python-beautifulsoup4" ,python-beautifulsoup4)
       ("python-furl" ,python-furl)
       ("python-sqlitedict" ,python-sqlitedict)
       ("python-schedule" ,python-schedule)
       ("python-tldextract" ,python-tldextract)
       ("dynaconf" ,dynaconf)))
    (synopsis "My self-quantification scripts")
    (description "My self-quantification scripts")
    (home-page "https://github.com/SqrtMinusOne/sqrt-data")
    (license license:gpl3)))
#+end_src

Also have to evaluate the variable with the definition for the =-f= flag.
#+begin_src scheme :tangle (my/org-prj-dir "sqrt-data.scm")
sqrt-data
#+end_src
** Server deployement
*** Docker
The server part uses Docker because I'm in love with Docker.

A Dockerfile for the program:
#+begin_src dockerfile :tangle (my/org-prj-dir "Dockerfile")
FROM python:3.10-buster
# Install sqrt-data
WORKDIR "sqrt_data/"
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY sqrt_data/ setup.py ./
RUN pip install .
ENV PYTHONPATH="$PYTHONPATH:/sqrt_data"

WORKDIR "/"
RUN mkdir /tmp/sqrt-data

CMD sqrt_data cron run-server-cron
#+end_src

A docker-compose file. Consists of 4 services:
- PostgresSQL database
- Metabase
- This application
- A backup service
#+begin_src yaml :tangle (my/org-prj-dir "docker-compose.yml")
version: "3.5"

services:
    postgres:
        restart: unless-stopped
        image: postgres
        container_name: "sqrt-data-postgres"
        ports:
            - 5432:5432
        networks:
            - postgres
        environment:
            POSTGRES_USER: postgres
            POSTGRES_PASSWORD: localdbpass
            POSTGRES_DB: data
        volumes:
            - postgres_data:/data/postgres
    metabase:
        container_name: "sqrt-data-metabase"
        restart: unless-stopped
        image: metabase/metabase
        ports:
            - 8083:3000
        networks:
            - postgres
        depends_on:
            - postgres
        environment:
            MB_DB_TYPE: postgres
            MB_DB_DBNAME: metabase
            MB_DB_PORT: 5432
            MB_DB_USER: postgres
            MB_DB_PASS: localdbpass
            MB_DB_HOST: postgres
    sqrt_data:
        container_name: "sqrt-data"
        build: .
        restart: unless-stopped
        networks:
            - postgres
        depends_on:
            - postgres
        volumes:
            - type: bind
              source: ./config.toml
              target: /config.toml
            - type: bind
              source: ~/logs-sync-debug
              target: /root/logs-sync-debug
    backups:
        image: prodrigestivill/postgres-backup-local
        restart: always
        volumes:
            - ./backups:/backups
        networks:
            - postgres
        depends_on:
            - postgres
        environment:
            - POSTGRES_HOST=postgres
            - POSTGRES_DB=data,metabase
            - POSTGRES_USER=postgres
            - POSTGRES_PASSWORD=localdbpass
            - POSTGRES_EXTRA_OPTS=-Fc -Z9
            - SCHEDULE=@daily
            - BACKUP_KEEP_DAYS=7
            - BACKUP_KEEP_WEEKS=4
            - BACKUP_KEEP_MONTHS=2
            - BACKUP_SUFFIX=.dump
            - HEALTHCHECK_PORT=8080

networks:
    postgres:
        driver: bridge

volumes:
    postgres_data:
#+end_src
*** Basic deployement instructions
The server deployment is as follows.
1. Clone the repository
2. Change the settings in =config.toml= and =docker-compose.yml= according to the setup. What has to be changed:
   - The folder with logs has to be mounted to the container. The mount path has to be setup in =config.toml= in =general.root=.
   - =database.host= should be set to "postgres"
   - It makes sense to change the password in =database.password= and in the compose file.
3. Create a database for metabase. To do that, run:
   #+begin_src bash :eval no
   docker-compose up postgres
   #+end_src

   Run psql in a separate shell:
   #+begin_src bash :eval no
   PGPASSWORD=localdbpass psql -h localhost -U postgres
   #+end_src

   And create a database:
   #+begin_src sql
   CREATE DATABASE metabase;
   #+end_src

   It may make sense to make a separate user for Metabase here.
4. Check if everything works correctly.
   - Metabase instance should be available at http://localhost:8083/
   - Run =docker exec -it sqrt-data /bin/bash=:
     - Check if =config.toml= and the logs folder are mounted correctly
     - =sqrt_data hash-list= has to work
     - =sqrt_data mpd load-library= has to work
