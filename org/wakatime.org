#+TITLE: WakaTime data
#+PROPERTY: header-args :mkdirp yes
#+PROPERTY: header-args:python :comments link
#+PROPERTY: PRJ-DIR ..

[[https://wakatime.com/dashboard][WakaTime]] is a free service to log the programming activity.

Unfortunately, the service is SaaS and the data is stored on the company's servers, so use it at your discretion.

There are two ways to access the data:
- The "official" API provides the full data, but required a subscription to access the records older than 2 weeks.
- The coding activity export provides the aggregated data for the entire usage history.

The latter is interesting enough, so I'm fine with using it.

Edit <2022-01-07 Fri>: WakaTime now allows to export the unprocessed data.

* Flow
:PROPERTIES:
:header-args:python: :tangle (my/org-prj-dir "sqrt_data_service/flows/wakatime/flow.py") :comments link
:END:

Import the packages:
#+begin_src python
import base64
import os
import requests
import pandas as pd
import json
from collections import deque
from prefect import task, flow, get_run_logger
from tqdm import tqdm

from sqrt_data_service.api import settings, DBConn, FileHasher
#+end_src

** Download dump
First, we have to get a WakaTime dump.

It seems like there's no way to initiate the export via the API, but somehow it's possible to download the dump.

The first task checks if the dump is available, and downloads it if so.
#+begin_src python
@task
def download_wakatime_dump():
    logger = get_run_logger()
    key = base64.b64encode(str.encode(settings["waka"]["api_key"])
                          ).decode('utf-8')
    headers = {'Authorization': f'Basic {key}'}
    r = requests.get(
        f'{settings["waka"]["api_url"]}/users/current/datadumps',
        headers=headers
    )
    data = r.json()['data']
    if len(data) == 0:
        logger.info('No WakaTime dumps found')
        return None

    dump_data = data[0]
    if dump_data['status'] != 'Completed':
        logger.info('Dump not completed')
        return None

    filename = f'wakatime-{dump_data["created_at"]}.json'
    path = os.path.join(
        os.path.expanduser(settings['general']['temp_data_folder']), filename
    )
    if os.path.exists(path):
        logger.info('File already downloaded')
        return path
    os.makedirs(
        os.path.expanduser(settings['general']['temp_data_folder']),
        exist_ok=True
    )

    dump = requests.get(dump_data['download_url'])
    with open(path, 'wb') as f:
        f.write(dump.content)
    logger.info('WakaTime dump downloaded to %s', filename)
    return path
#+end_src

** Parse dump
Next, convert the dump to the set of DataFrames. Here's an outline of the dump structure:

The dump is structured like this:
- =user= - information about the user
- =range= - looks like the start and the end of observations
- =days= - An array of aggregated data per each day.

An entry for one day is a dictionary with the following values:
- =date= - the date in the =YYYY-MM-DD= format
- =categories= - here starts the data
- =dependencies=
- =editors=
- =languages=
- =machines=
- =operating_systems=
- =grand_total=
- =projects=

A single datum normally consists of a few common time-related fields:
- =decimal= - hours and minutes
- =digital= - "HH:MM"
- =hours=
- =minutes=
- =total_seconds=
- =text=

=grand_total= is an aggredate datum for the given day.

=categories=, =dependencies=, =editors=, =languages=, =machines= and =operating_systems= are arrays of such datums, extended with the following fields:
- =name= - the name of editor/category/etc
- =percent= - percent for this day

=projects= is a array of projects, which were active during the day. A project consists of the following fields
- =name= - the name of the project
- =branches=
- =categories=
- =dependencies=
- =editors=
- =entities= - files!
- =grand_total=
- =languages=
- =machines=
- =operating_systems=

=grand_total= is once again an aggregate datum for the given project for the given day.

=branches=, =categories=, =dependencies=, =editors=, =entries=, =languages=, =machines= and =operating_systems= are arrays of such "extended datums".

For the purposes of my analysis, I merge the data from the =projects= arrays.
#+begin_src python
@task
def parse_wakatime_dump(data):
    deques = {}

    for day in tqdm(data['days']):
        date = day['date']
        for project in day['projects']:
            name = project['name']
            for key, date_data in project.items():
                if key == 'name':
                    continue
                try:
                    data_deque = deques[key]
                except KeyError:
                    data_deque = deque()
                    deques[key] = data_deque
                if key == 'grand_total':
                    data_deque.append(
                        {
                            "date": date,
                            "project": name,
                            **date_data
                        }
                    )
                else:
                    for datum in date_data:
                        data_deque.append(
                            {
                                "date": date,
                                "project": name,
                                **datum
                            }
                        )

    dfs = {name: pd.DataFrame(data) for name, data in deques.items()}
    for name, df in dfs.items():
        df['total_minutes'] = df['total_seconds'] / 60
        df['date'] = pd.to_datetime(df['date'])
        # df['date'] = df['date'].apply(lambda dt: dt.date())
        df = df.drop(['total_seconds'], axis=1)
        dfs[name] = df
    return dfs
#+end_src

In the end we have 9 DataFrames of "extended datums" with =date= and =project= attributes.

** Store dump
Finally, store the data into database:
#+begin_src python
@task
def store_wakatime_dump(dfs):
    DBConn.create_schema(settings['waka']['schema'])
    for name, df in tqdm(dfs.items()):
        df.to_sql(
            name,
            schema=settings['waka']['schema'],
            con=DBConn.engine,
            if_exists='replace'
        )
        print(df)
#+end_src

Putting all of this together:
#+begin_src python
@flow
def wakatime():
    DBConn()
    hasher = FileHasher()
    logger = get_run_logger()

    dump_file = download_wakatime_dump()
    if dump_file is None:
        return

    if hasher.is_updated(dump_file) is False:
        logger.info('Dump already processed')
        return

    with open(dump_file, 'r') as f:
        data = json.load(f)

    dfs = parse_wakatime_dump(data)
    store_wakatime_dump(dfs)
    hasher.save_hash(dump_file)
#+end_src

Execute the flow:
#+begin_src python
if __name__ == '__main__':
    wakatime()
#+end_src

* Deploy
:PROPERTIES:
:header-args:python: :tangle (my/org-prj-dir "sqrt_data_service/flows/wakatime/deploy.py") :comments link
:END:

Create the deployment:
#+begin_src python
from prefect.deployments import Deployment
from prefect.orion.schemas.schedules import CronSchedule

from sqrt_data_service.api import settings
from .flow import wakatime

def create_deploy():
    deployment = Deployment.build_from_flow(
        flow=wakatime,
        name="wakatime-dump",
        work_queue_name=settings.prefect.queue,
        schedule=(CronSchedule(cron="0 0 * * *"))
    )
    deployment.apply()

if __name__ == '__main__':
    create_deploy()
#+end_src

Run the following:
#+begin_src bash :tangle no
python -m sqrt_data_service.flows.wakatime.deploy
#+end_src

To create a deployment until I've found a better way.
